
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tensorflow-based Inference Engine &#8212; RocketML 1.0.0 (RocketML Confidential) documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="rmltf execution" href="rmltf_execution.html" />
    <link rel="prev" title="rmldnn installation" href="rmldnn_installation.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="tensorflow-based-inference-engine">
<h1>Tensorflow-based Inference Engine<a class="headerlink" href="#tensorflow-based-inference-engine" title="Permalink to this headline">¶</a></h1>
<p><code class="code docutils literal notranslate"><span class="pre">rmltf</span></code> is RocketML’s Tensorflow-based inference engine. Given a trained model and an input dataset, an inference job can be
launched from the command line with the command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rmltf --config<span class="o">=</span>&lt;json_config_file&gt;
</pre></div>
</div>
<p>All aspects of how the run is configured must be specified in the <em>json</em> file passed in the <code class="code docutils literal notranslate"><span class="pre">--config</span></code> command-line argument.
The rest of this document will discuss all supported configuration parameters for this application.</p>
<div class="section" id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h2>
<p>The json file must contain one single object named <code class="code docutils literal notranslate"><span class="pre">neural_network</span></code>, inside which all configuration will reside:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
    <span class="s2">&quot;neural_network&quot;</span>: <span class="o">{</span>
        <span class="s2">&quot;checkpoints&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>,
        <span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The <code class="code docutils literal notranslate"><span class="pre">neural_network</span></code> object contains sub-objects (sections) which will be discussed next.</p>
<div class="section" id="checkpoints-section">
<h3>Checkpoints section<a class="headerlink" href="#checkpoints-section" title="Permalink to this headline">¶</a></h3>
<p>The path to the Tensorflow model directory must be passed to the application in the <code class="code docutils literal notranslate"><span class="pre">checkpoints::load</span></code> parameter, as for example,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;checkpoints&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;load&quot;</span>: <span class="s2">&quot;./path/to/tf_model/&quot;</span>
<span class="o">}</span>
</pre></div>
</div>
<p>Models trained with Keras and saved as <code class="code docutils literal notranslate"><span class="pre">.h5</span></code> files must be converted to Tensorflow format before running <cite>rmltf</cite>.
This can be done with the following Python code:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>from tensorflow import keras

<span class="nv">model</span> <span class="o">=</span> keras.models.load_model<span class="o">(</span><span class="s1">&#39;keras_model.h5&#39;</span><span class="o">)</span>
model.save<span class="o">(</span><span class="s1">&#39;./tf_model/&#39;</span>, <span class="nv">save_format</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="o">)</span>
</pre></div>
</div>
</div>
<div class="section" id="data-section">
<h3>Data section<a class="headerlink" href="#data-section" title="Permalink to this headline">¶</a></h3>
<p>In this section, the input data used for inference is configured, as well as several aspects of the data loader.
The only data type currently supported in <cite>rmltf</cite> is NumPy.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;type&quot;</span>: <span class="s2">&quot;numpy&quot;</span>,
    <span class="s2">&quot;test_input_path&quot;</span>: <span class="s2">&quot;/path/to/input/data/&quot;</span>,
    <span class="s2">&quot;test_batch_size&quot;</span>: <span class="m">16</span>,
    <span class="s2">&quot;preload&quot;</span>: <span class="nb">true</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The following parameters can be set:</p>
<ul class="simple">
<li><p><strong>type</strong>: Input data type. Must be set to <code class="code docutils literal notranslate"><span class="pre">numpy</span></code> in <cite>rmltf</cite></p></li>
<li><p><strong>test_input_path</strong>: Path to the input data file (single .npy or .npz file) or directory. See below for more details.</p></li>
<li><p><strong>test_batch_size</strong>: Number of inference samples per mini-batch (default is 64).</p></li>
<li><p><strong>preload</strong>: Whether samples will be read up-front from disk and loaded from memory during inference (default is <em>false</em>).</p></li>
</ul>
<p>Each numpy array becomes a sample to be forwarded through the network for inference.
One <code class="code docutils literal notranslate"><span class="pre">.npy</span></code> file contains a single numpy array, while one <code class="code docutils literal notranslate"><span class="pre">.npz</span></code> file might contain multiple numpy arrays.
If a directory with multiple <code class="code docutils literal notranslate"><span class="pre">.npy</span></code> or <code class="code docutils literal notranslate"><span class="pre">.npz</span></code> files is passed, arrays are extracted from all files and added to the
list of input samples.</p>
<div class="section" id="slicers-sub-section">
<h4>Slicers sub-section<a class="headerlink" href="#slicers-sub-section" title="Permalink to this headline">¶</a></h4>
<p>The <strong>numpy</strong> data loader also supports extracting the input samples from a single large numpy array by chopping it off into smaller
blocks of configurable sizes. The samples obtained can have equal or lower dimensionality as the original data, as long as the neural
network can handle their shapes. For example, if the input numpy array is a 3D block of shape <img class="math" src="_images/math/e83927553b2140bc4a007e656843091ae7bf8b79.png" alt="(H,W,D)"/>,
one could chop it into smaller blocks of shape <img class="math" src="_images/math/4e511e60dc95aa27223c973c409097b809c24520.png" alt="(h,w,d"/>), where <img class="math" src="_images/math/7659d34d232c2f08af76f7b0193285d7e4865c4d.png" alt="h \le H"/>, <img class="math" src="_images/math/ce90168eeb20a2df7ba10b459456838637771845.png" alt="w \le W"/> and <img class="math" src="_images/math/fcfc4db70bb523ba029b56d2c5209fd4219f569a.png" alt="d \le D"/>,
or slice it into 2D tiles along the <img class="math" src="_images/math/86d65811848faa07e28196570f82cd1aa83907a8.png" alt="xy"/>-plane with shape <img class="math" src="_images/math/9ec1618a45dbda24c96f5638319fe0efd118f093.png" alt="(h,w)"/>,
or even extract 1D lines of length <img class="math" src="_images/math/188339533baac6277e73d2071f1b38104be43941.png" alt="w &lt; W"/> along the <img class="math" src="_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/>-axis.
Multiple slice sets can be defined, each with its own slice size and orientation (the dimensionality of slices across all sets
must be the same, though, since the neural network is common to all). The configuration below shows an example of how to extract
2D samples from a 3D input array using 2 slice sets:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;data&quot;</span>: <span class="s2">&quot;/path/to/input_block.npy&quot;</span>,
    <span class="s2">&quot;slicers&quot;</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>:               <span class="s2">&quot;yz-slices&quot;</span>,
            <span class="s2">&quot;sizes&quot;</span>:              <span class="o">[</span><span class="m">1</span>, <span class="m">131</span>, <span class="m">1001</span><span class="o">]</span>,
            <span class="s2">&quot;padded_sizes&quot;</span>:       <span class="o">[</span><span class="m">1</span>, <span class="m">144</span>, <span class="m">1008</span><span class="o">]</span>,
            <span class="s2">&quot;discard_remainders&quot;</span>: false,
            <span class="s2">&quot;transpose&quot;</span>:          <span class="nb">false</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>:               <span class="s2">&quot;xz-slices&quot;</span>,
            <span class="s2">&quot;sizes&quot;</span>:              <span class="o">[</span><span class="m">540</span>, <span class="m">1</span>, <span class="m">1001</span><span class="o">]</span>,
            <span class="s2">&quot;padded_sizes&quot;</span>:       <span class="o">[</span><span class="m">560</span>, <span class="m">1</span>, <span class="m">1008</span><span class="o">]</span>,
            <span class="s2">&quot;discard_remainders&quot;</span>: false,
            <span class="s2">&quot;transpose&quot;</span>:          <span class="nb">true</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The following options can be set:</p>
<ul class="simple">
<li><p><strong>name</strong>: Slice set name (optional)</p></li>
<li><p><strong>sizes</strong>: Slice sizes (required). Expects N elements for N-dimensional input data. Setting an element to 1 flattens the slice along that dimension,
reducing the dimensionality of the input samples into the network.</p></li>
<li><p><strong>padding</strong>: Symmetric padding to be added along each dimension (defaults to zero). If <img class="math" src="_images/math/4c0880addc1c05a04ecaed29678ba0abdc919ac2.png" alt="\textrm{sizes=} [h,w,d]"/> and
<img class="math" src="_images/math/2122eb987fa2d045b29c1c19a8526a2e7387d4e6.png" alt="\textrm{padding=}[p_x, p_y, p_z]"/>, then slices will have shape <img class="math" src="_images/math/be25f8c404fcaa937396b72181f0555ea7c166db.png" alt="(h + 2 p_x, w + 2 p_y, d + 2 p_z)"/>.
Cannot be specified together with <cite>padded_sizes</cite>.</p></li>
<li><p><strong>padded_sizes</strong>: Total slice size after padding (defaults to <cite>sizes</cite>). Useful in case the desired padding is asymmetric.
Cannot be specified together with <cite>padding</cite>.</p></li>
<li><p><strong>discard_remainders</strong>: Whether to discard regions of the original input data which are left over after division by the slice sizes (default is <cite>false</cite>).</p></li>
<li><p><strong>transpose</strong>: Whether to transpose each slice before and after network traversal. Only valid for 2D slices (default is <cite>false</cite>).</p></li>
</ul>
<p>The inference process, including the addition and removal of padding (as well as optional slice transposition), is
depicted in the figure below:</p>
<a class="reference internal image-reference" href="_images/slicer_padding.png"><img alt="slicer_padding.png" src="_images/slicer_padding.png" style="width: 600px;" /></a>
<p><strong>HDF5 output writing</strong></p>
<p>The predictions obtained by running inferences on the slices can be assembled back into a multi-dimensional array and saved to disk
as an HDF5 file. Each slice set will result in one dataset in the HDF5 data-structure.
In order to enable HDF5 writing, set the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;hdf5_outfile&quot;</span>: <span class="s2">&quot;prediction.h5&quot;</span>
    ...
<span class="o">}</span>
</pre></div>
</div>
<p>The process of writing data into the HDF5 file is performed in parallel (in case of multi-process execution)
and asynchronously, i.e., it happens concurrently with inference in order to maximize throughput.
The entire infrastructure for data slicing, inferencing and assembling is depicted in the figure below.</p>
<a class="reference internal image-reference" href="_images/slicer_flow.png"><img alt="slicer_flow.png" src="_images/slicer_flow.png" style="width: 600px;" /></a>
<p><strong>Restrictions:</strong></p>
<ul class="simple">
<li><p>The input numpy array must have no channel dimension (i.e., the data must be single-channel with only spatial dimensions).</p></li>
<li><p>The shape of the output tensor produced by the network must be equal to the input shape plus en extra channel dimension.</p></li>
<li><p>Only 2D slices can be transposed.</p></li>
</ul>
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">RocketML</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="deep_neural_networks.html">Deep Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="rmldnn_installation.html"><em>rmldnn</em> installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tensorflow-based Inference Engine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#configuration">Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#checkpoints-section">Checkpoints section</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-section">Data section</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#slicers-sub-section">Slicers sub-section</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rmltf_execution.html"><em>rmltf</em> execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="rmltf_installation.html"><em>rmltf</em> installation</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="rmldnn_installation.html" title="previous chapter"><em>rmldnn</em> installation</a></li>
      <li>Next: <a href="rmltf_execution.html" title="next chapter"><em>rmltf</em> execution</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, RocketML.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/rmltf_inference_engine.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>